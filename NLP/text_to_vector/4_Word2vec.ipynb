{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2vec\n",
    "\n",
    "This is a supervised ML model.\n",
    "\n",
    "The most imporant concept in respect of word2vec is word vectorization using feature representation. To understand it intuitivly we can say, that all words during the training stage is represented as vectors, that receive value, by understanding how they are related in regards of the features or meaning that they have.\n",
    "\n",
    "For example we can say that - [ king - boy + girl = queen]\n",
    "\n",
    "Cosine similarity is super imporant aswell here, it represents the angle between two vectors. \n",
    "\n",
    "Distance = 1 - cosine similarity\n",
    "\n",
    "For example if the angle between two vectors are 90degrees, we can say that both of them are completely different, thus has a different meaning, boy and girl could have such angle.\n",
    "\n",
    "### training process\n",
    "\n",
    "If we are talking about CBOW method, it uses a 'sliding window' technique. It takes the middle word as the output and the words around it is considered as the input in this case. \n",
    "\n",
    "In the initial steps the first input vector is can be given as a one-hot-encoding vector.\n",
    "\n",
    "The input layer in case we are using window_size as 5, then the input layer would be 4 neurons, that are of dim 7, if the sentence is 7 words long. Then the hidden layer, in case our network is simple and has only one layer, is 5 dim long vector. And then the output layer would have 7 dimensions.\n",
    "\n",
    "Feature representation is basically the window size, thus the bigger it gets the better information we will receive.\n",
    "\n",
    "And to mention, this is using backpropogation and loss function minimization.\n",
    "\n",
    "The skipgram method is basically reversed version of the CBOW method, input in this case would be the middle word, that is represented by a vector that is lenght of vocabulary.\n",
    "\n",
    "### when to apply which\n",
    "\n",
    "In the research it says that CBOW is better when the dataset is rather small and the skipgram is opposite, when the dataset is huge.\n",
    "\n",
    "Both of them are reliant on dataset size, window size (the bigger the better, since feature representation gets more indepht)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim \n",
    "import gensim.downloader as api\n",
    "from gensim.models import Word2Vec, KeyedVectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main functions:\n",
    "- most_similar()\n",
    "- similarity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "king = wv['king']\n",
    "king.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kings', 0.7138046622276306),\n",
       " ('queen', 0.6510956287384033),\n",
       " ('monarch', 0.6413194537162781),\n",
       " ('crown_prince', 0.6204219460487366),\n",
       " ('prince', 0.6159993410110474),\n",
       " ('sultan', 0.5864824056625366),\n",
       " ('ruler', 0.5797566771507263),\n",
       " ('princes', 0.5646552443504333),\n",
       " ('Prince_Paras', 0.5432944297790527),\n",
       " ('throne', 0.5422105193138123)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.most_similar('king')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12784153"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wv.similarity('kind', 'queen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('king', 0.8449392318725586),\n",
       " ('queen', 0.7300516366958618),\n",
       " ('monarch', 0.6454660296440125),\n",
       " ('princess', 0.6156251430511475),\n",
       " ('crown_prince', 0.5818676948547363),\n",
       " ('prince', 0.5777117609977722),\n",
       " ('kings', 0.5613663792610168),\n",
       " ('sultan', 0.5376776456832886),\n",
       " ('Queen_Consort', 0.5344247817993164),\n",
       " ('queens', 0.5289887189865112)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec = wv['king'] - wv['man'] + wv['woman']\n",
    "wv.most_similar(vec)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
